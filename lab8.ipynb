{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6955c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in c:\\users\\saram\\anaconda3\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\saram\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\saram\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saram\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saram\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saram\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saram\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instala pyreadstat para poder leer archivos .sav d\n",
    "%pip install pyreadstat pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SparkSession (from versions: none)\n",
      "ERROR: No matching distribution found for SparkSession\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyspark.sql (from versions: none)\n",
      "ERROR: No matching distribution found for pyspark.sql\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b30ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 27\n",
      "\n",
      "✅ UNIÓN COMPLETA.\n",
      "El archivo maestro se creó en: /Users/saram/OneDrive/Documents/Data science/lab8/data\\BASE_MAESTRA.xlsx\n",
      "Registros totales en el maestro: 228968\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ⚠️ REEMPLACE ESTA RUTA con la ruta ABSOLUTA de su carpeta local\n",
    "BASE_DIR = \"/Users/saram/OneDrive/Documents/Data science/lab8/data\" \n",
    "\n",
    "# Nombre del archivo maestro que crearemos\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, \"BASE_MAESTRA.xlsx\")\n",
    "\n",
    "# 1. Busca todos los archivos .xlsx\n",
    "all_excel_files = glob.glob(os.path.join(BASE_DIR, \"*.xlsx\"))\n",
    "\n",
    "if not all_excel_files:\n",
    "    print(\"❌ ERROR: No se encontraron archivos .xlsx en la ruta especificada.\")\n",
    "else:\n",
    "    print(f\"Archivos encontrados: {len(all_excel_files)}\")\n",
    "    \n",
    "    # 2. Lee y une todos los DataFrames\n",
    "    master_list = []\n",
    "    \n",
    "    # Agrega una columna para rastrear el origen (opcional, pero útil)\n",
    "    for f in all_excel_files:\n",
    "        df = pd.read_excel(f)\n",
    "        df['ORIGEN_ARCHIVO'] = os.path.basename(f)\n",
    "        master_list.append(df)\n",
    "\n",
    "    # Une todos los DataFrames en uno solo\n",
    "    master_df = pd.concat(master_list, ignore_index=True)\n",
    "    \n",
    "    # 3. Guarda el archivo maestro en la misma carpeta\n",
    "    master_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n✅ UNIÓN COMPLETA.\")\n",
    "    print(f\"El archivo maestro se creó en: {OUTPUT_FILE}\")\n",
    "    print(f\"Registros totales en el maestro: {len(master_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bb63bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "relative path can't be expressed as a file URI",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. DEFINICIÓN MANUAL DE LA RUTA MAESTRA (Fijando el formato con pathlib)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Esto garantiza que la ruta sea interpretada correctamente por la JVM de Spark.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Construimos la ruta y la convertimos en un URI de archivo para máxima compatibilidad.\u001b[39;00m\n\u001b[32m     22\u001b[39m MASTER_PATH = Path(BASE_DIR) / \u001b[33m\"\u001b[39m\u001b[33mBASE_MAESTRA.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m MASTER_FILE_URI = \u001b[43mMASTER_PATH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 4. Iniciar la sesión de Spark (con el conector Excel)\u001b[39;00m\n\u001b[32m     26\u001b[39m spark = SparkSession.builder \\\n\u001b[32m     27\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mPNC_Lab_Maestro\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     28\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.jars.packages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcom.crealytics:spark-excel_2.12:0.13.5\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     29\u001b[39m     .getOrCreate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saram\\anaconda3\\Lib\\pathlib.py:467\u001b[39m, in \u001b[36mPurePath.as_uri\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the path as a 'file' URI.\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_absolute():\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mrelative path can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be expressed as a file URI\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    469\u001b[39m drive = \u001b[38;5;28mself\u001b[39m.drive\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(drive) == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m drive[\u001b[32m1\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# It's a path on a local drive => 'file:///c:/a/b'\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: relative path can't be expressed as a file URI"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count\n",
    "\n",
    "# =======================================================================\n",
    "# ⚠️ 1. CONFIGURACIÓN LOCAL Y RUTA\n",
    "# =======================================================================\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/hadoop\" \n",
    "\n",
    "# 1. Iniciar la sesión de Spark con la configuración para leer Excel\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PNC_Lab_Maestro\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. REEMPLACE ESTA RUTA con la ruta ABSOLUTA de su carpeta local (LA MISMA QUE ANTES)\n",
    "BASE_DIR = \"/Users/saram/OneDrive/Documents/Data science/lab8/data\" \n",
    "# 3. DEFINICIÓN MANUAL DE LA RUTA MAESTRA (Fijando el formato con pathlib)\n",
    "# Esto garantiza que la ruta sea interpretada correctamente por la JVM de Spark.\n",
    "# Construimos la ruta y la convertimos en un URI de archivo para máxima compatibilidad.\n",
    "MASTER_PATH = Path(BASE_DIR) / \"BASE_MAESTRA.xlsx\"\n",
    "MASTER_FILE_URI = MASTER_PATH.as_uri()\n",
    "\n",
    "# 4. Iniciar la sesión de Spark (con el conector Excel)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PNC_Lab_Maestro\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# =======================================================================\n",
    "# 2. CARGA Y DIVISIÓN DEL ARCHIVO MAESTRO\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n--- INICIO DE CARGA Y FILTRADO ---\")\n",
    "\n",
    "# Cargar el archivo maestro único\n",
    "try:\n",
    "    master_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(MASTER_FILE)\n",
    "    \n",
    "    # Estandariza a MAYÚSCULAS para evitar errores de columna\n",
    "    for c in master_df.columns:\n",
    "        if c.upper() != c:\n",
    "            master_df = master_df.withColumnRenamed(c, c.upper())\n",
    "            \n",
    "    # Muestra todas las columnas del archivo maestro (ayuda a la clasificación)\n",
    "    print(\"\\nColumnas del archivo maestro (use estos nombres para el filtrado):\")\n",
    "    print(master_df.columns)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: No se pudo cargar el archivo maestro. Asegúrese de que existe en {MASTER_FILE}\")\n",
    "    spark.stop()\n",
    "    exit() # Detiene la ejecución si falla la carga\n",
    "\n",
    "# --- DEFINICIÓN DE COLUMNAS CLAVE POR BASE (AJUSTE ESTOS NOMBRES SI ES NECESARIO) ---\n",
    "\n",
    "# Columna única que solo existe en la base de Vehículos (ej. una ID que no está en las otras)\n",
    "COLUMNAS_VEHICULOS = ['MARCA', 'COLOR', 'TIPO_VEHICULO', 'SEXO_CONDUCTOR'] \n",
    "\n",
    "# Columna única que solo existe en la base de Víctimas\n",
    "COLUMNAS_VICTIMAS = ['CONDICION', 'GRUPO_EDAD', 'SEXO_VICTIMA'] \n",
    "\n",
    "# Columna clave que debe estar en todas las bases (se usa para uniones)\n",
    "COLUMNAS_JOIN = ['AÑO', 'MES', 'DIA_SEMANA', 'HORA', 'DEPARTAMENTO', 'TIPO_ACCIDENTE'] \n",
    "\n",
    "# --- FILTRADO PARA CREAR LOS 3 DATAFRAMES ---\n",
    "\n",
    "# 1. HECHOS (Contiene columnas de JOIN y NINGUNA de las columnas ÚNICAS de Vehículos o Víctimas)\n",
    "hechos_cols = [c for c in master_df.columns if c not in COLUMNAS_VEHICULOS and c not in COLUMNAS_VICTIMAS]\n",
    "hechos_df = master_df.select(*hechos_cols).distinct() \n",
    "\n",
    "# 2. VEHÍCULOS (Contiene columnas de JOIN y las columnas ÚNICAS de Vehículos)\n",
    "vehiculos_cols = COLUMNAS_JOIN + [c for c in master_df.columns if c in COLUMNAS_VEHICULOS]\n",
    "vehiculos_df = master_df.select(*vehiculos_cols).distinct()\n",
    "\n",
    "# 3. VÍCTIMAS (Contiene columnas de JOIN y las columnas ÚNICAS de Víctimas)\n",
    "victimas_cols = COLUMNAS_JOIN + [c for c in master_df.columns if c in COLUMNAS_VICTIMAS]\n",
    "victimas_df = master_df.select(*victimas_cols).distinct()\n",
    "\n",
    "\n",
    "print(\"\\n--- RESUMEN DE CARGA FINAL ---\")\n",
    "print(f\"✅ HECHOS_DF cargado. Registros: {hechos_df.count()}\")\n",
    "print(f\"✅ VEHICULOS_DF cargado. Registros: {vehiculos_df.count()}\")\n",
    "print(f\"✅ VICTIMAS_DF cargado. Registros: {victimas_df.count()}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# SECCIÓN: ANÁLISIS EXPLORATORIO (EJERCICIOS 1-7)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 1: Mostrar el esquema de los 3 DataFrames.\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 1: ESQUEMAS ---\")\n",
    "print(\"=\"*50)\n",
    "hechos_df.printSchema()\n",
    "vehiculos_df.printSchema()\n",
    "victimas_df.printSchema()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 2: Mostrar la cantidad total de registros de cada DataFrame.\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 2: CONTEO DE REGISTROS ---\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Hechos: {hechos_df.count()} registros\")\n",
    "print(f\"Vehículos: {vehiculos_df.count()} registros\")\n",
    "print(f\"Víctimas: {victimas_df.count()} registros\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 3: Mostrar la cantidad de nulos por columna para cada DataFrame.\n",
    "# -----------------------------------------------------\n",
    "def count_nulls(df, df_name):\n",
    "    \"\"\"Calcula el conteo de nulos por columna.\"\"\"\n",
    "    print(f\"\\n--- NULOS en {df_name} ---\")\n",
    "    null_counts_df = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "        for c in df.columns\n",
    "    ])\n",
    "    \n",
    "    null_counts = null_counts_df.collect()[0].asDict()\n",
    "    has_nulls = False\n",
    "    for k, v in null_counts.items():\n",
    "        if v > 0:\n",
    "            print(f\"  {k}: {v} nulos\")\n",
    "            has_nulls = True\n",
    "    \n",
    "    if not has_nulls:\n",
    "        print(\"  (No hay nulos en las columnas seleccionadas.)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 3: CONTEO DE NULOS POR COLUMNA ---\")\n",
    "print(\"=\"*50)\n",
    "count_nulls(hechos_df, \"Hechos\")\n",
    "count_nulls(vehiculos_df, \"Vehículos\")\n",
    "count_nulls(victimas_df, \"Víctimas\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 4: Mostrar un describe (resumen estadístico) de los 3 DataFrames.\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 4: RESUMEN ESTADÍSTICO (DESCRIBE) ---\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nHECHOS:\")\n",
    "hechos_df.describe().show(truncate=False)\n",
    "print(\"\\nVEHÍCULOS:\")\n",
    "vehiculos_df.describe().show(truncate=False)\n",
    "print(\"\\nVÍCTIMAS:\")\n",
    "victimas_df.describe().show(truncate=False)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 5: Cantidad de tipos de accidentes únicos en Hechos.\n",
    "# -----------------------------------------------------\n",
    "tipos_unicos = hechos_df.select('TIPO_ACCIDENTE').distinct().count()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"--- EJERCICIO 5: TIPOS DE ACCIDENTES ÚNICOS ---\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Cantidad de Tipos de Accidente Únicos: {tipos_unicos}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 6: Conteo de accidentes por día de la semana.\n",
    "# -----------------------------------------------------\n",
    "accidentes_por_dia = hechos_df.groupBy('DIA_SEMANA').agg(\n",
    "    count('*').alias('TOTAL_ACCIDENTES')\n",
    ").orderBy(col('TOTAL_ACCIDENTES').desc())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 6: CONTEO POR DÍA DE LA SEMANA ---\")\n",
    "print(\"=\"*50)\n",
    "accidentes_por_dia.show(truncate=False)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EJERCICIO 7: Accidentes por mes del año.\n",
    "# -----------------------------------------------------\n",
    "accidentes_por_mes = hechos_df.groupBy('MES').agg(\n",
    "    count('*').alias('TOTAL_ACCIDENTES')\n",
    ").orderBy('MES')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- EJERCICIO 7: ACCIDENTES POR MES ---\")\n",
    "print(\"=\"*50)\n",
    "accidentes_por_mes.show(truncate=False)\n",
    "\n",
    "# Detener la sesión de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edb202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
